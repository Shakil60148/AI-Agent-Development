{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Project: Build a Basic AI-Powered FAQ Bot"
      ],
      "metadata": {
        "id": "5qQjBqTQnuWK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PydanticAI FAQ Bot - Module 1 Project\n",
        "# AI-Powered FAQ Bot using Ollama LLM and Embedding Models\n",
        "\n",
        "# SETUP AND INSTALLATIONS\n"
      ],
      "metadata": {
        "id": "Xc_ApoS1oANj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Ollama in Colab\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "\n",
        "# Start Ollama server in background\n",
        "import subprocess\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Start Ollama server\n",
        "print(\"üîÑ Starting Ollama server...\")\n",
        "ollama_process = subprocess.Popen(['ollama', 'serve'],\n",
        "                                  stdout=subprocess.DEVNULL,\n",
        "                                  stderr=subprocess.DEVNULL)\n",
        "\n",
        "# Wait for server to start\n",
        "time.sleep(5)\n",
        "print(\"‚úÖ Ollama server started!\")\n",
        "\n",
        "# Install required packages\n",
        "!pip install -q requests beautifulsoup4 numpy scikit-learn ollama python-dotenv\n",
        "!pip install -q sentence-transformers chromadb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K3Z0D4wzuclh",
        "outputId": "4d2196d2-496a-4706-f91e-0b617e0524b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "######################################################################## 100.0%\n",
            ">>> Creating ollama user...\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n",
            "üîÑ Starting Ollama server...\n",
            "‚úÖ Ollama server started!\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m19.5/19.5 MB\u001b[0m \u001b[31m105.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m76.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m65.8/65.8 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m55.7/55.7 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m118.5/118.5 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m196.2/196.2 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m71.2/71.2 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m121.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m97.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m56.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m85.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m103.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m453.1/453.1 kB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IMPORT LIBRARIES"
      ],
      "metadata": {
        "id": "4z1M-JhuoOMD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import numpy as np\n",
        "import json\n",
        "import re\n",
        "from typing import List, Dict, Any\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import ollama\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import chromadb\n",
        "from chromadb.utils import embedding_functions\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"‚úÖ All libraries imported successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9AH2wh9UueVP",
        "outputId": "7c61b258-d350-48ee-876b-84dc87298c52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ All libraries imported successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OLLAMA SETUP AND MODEL PULLING (WITH FALLBACK)"
      ],
      "metadata": {
        "id": "KhBUrtlDoYdz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pull required models from Ollama with error handling\n",
        "print(\"üîÑ Pulling Ollama models...\")\n",
        "\n",
        "MODELS_READY = False\n",
        "\n",
        "try:\n",
        "    # Test if Ollama is accessible\n",
        "    ollama.list()\n",
        "    print(\"‚úÖ Ollama is accessible!\")\n",
        "\n",
        "    # Pull LLM model for text generation\n",
        "    print(\"Pulling llama3.2:3b model...\")\n",
        "    ollama.pull('llama3.2:3b')\n",
        "    print(\"‚úÖ llama3.2:3b model pulled successfully!\")\n",
        "\n",
        "    # Pull embedding model\n",
        "    print(\"Pulling nomic-embed-text model...\")\n",
        "    ollama.pull('nomic-embed-text')\n",
        "    print(\"‚úÖ nomic-embed-text model pulled successfully!\")\n",
        "\n",
        "    MODELS_READY = True\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error with Ollama: {e}\")\n",
        "    print(\"üîÑ Setting up fallback embedding model...\")\n",
        "\n",
        "    # Fallback to sentence-transformers\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "\n",
        "    try:\n",
        "        # Load a lightweight embedding model\n",
        "        fallback_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "        print(\"‚úÖ Fallback embedding model loaded successfully!\")\n",
        "        MODELS_READY = True\n",
        "    except Exception as fallback_error:\n",
        "        print(f\"‚ùå Fallback model error: {fallback_error}\")\n",
        "        print(\"Please ensure you have internet connection for model download.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sg5_y4rGunGo",
        "outputId": "71429fcc-f887-4d53-ef8f-52115d7fe128"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ Pulling Ollama models...\n",
            "‚úÖ Ollama is accessible!\n",
            "Pulling llama3.2:3b model...\n",
            "‚úÖ llama3.2:3b model pulled successfully!\n",
            "Pulling nomic-embed-text model...\n",
            "‚úÖ nomic-embed-text model pulled successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WEB SCRAPING PYDANTIC AI DOCUMENTATION"
      ],
      "metadata": {
        "id": "BfBW1Ao6ofOD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_pydantic_ai_content(url: str) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Scrape content from PydanticAI documentation website\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(f\"üîÑ Scraping content from: {url}\")\n",
        "\n",
        "        # Send GET request with headers to mimic browser\n",
        "        headers = {\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "        }\n",
        "\n",
        "        response = requests.get(url, headers=headers, timeout=30)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        # Parse HTML content\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        # Extract main content areas\n",
        "        content_data = {\n",
        "            'title': '',\n",
        "            'sections': [],\n",
        "            'full_text': ''\n",
        "        }\n",
        "\n",
        "        # Extract title\n",
        "        title = soup.find('title')\n",
        "        if title:\n",
        "            content_data['title'] = title.get_text().strip()\n",
        "\n",
        "        # Extract main content sections\n",
        "        main_content = soup.find('main') or soup.find('article') or soup.find('div', class_='content')\n",
        "\n",
        "        if main_content:\n",
        "            # Extract all text content\n",
        "            full_text = main_content.get_text(separator=' ', strip=True)\n",
        "            content_data['full_text'] = full_text\n",
        "\n",
        "            # Extract sections with headings\n",
        "            headings = main_content.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n",
        "\n",
        "            for heading in headings:\n",
        "                section_text = heading.get_text().strip()\n",
        "\n",
        "                # Get content following the heading\n",
        "                next_content = []\n",
        "                for sibling in heading.next_siblings:\n",
        "                    if sibling.name in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:\n",
        "                        break\n",
        "                    if hasattr(sibling, 'get_text'):\n",
        "                        text = sibling.get_text(strip=True)\n",
        "                        if text:\n",
        "                            next_content.append(text)\n",
        "\n",
        "                if next_content:\n",
        "                    content_data['sections'].append({\n",
        "                        'heading': section_text,\n",
        "                        'content': ' '.join(next_content)\n",
        "                    })\n",
        "\n",
        "        # If no structured content found, extract all paragraphs\n",
        "        if not content_data['sections']:\n",
        "            paragraphs = soup.find_all('p')\n",
        "            for i, p in enumerate(paragraphs):\n",
        "                text = p.get_text(strip=True)\n",
        "                if text and len(text) > 50:  # Only include substantial paragraphs\n",
        "                    content_data['sections'].append({\n",
        "                        'heading': f'Section {i+1}',\n",
        "                        'content': text\n",
        "                    })\n",
        "\n",
        "        print(f\"‚úÖ Successfully scraped {len(content_data['sections'])} sections\")\n",
        "        print(f\"üìÑ Total content length: {len(content_data['full_text'])} characters\")\n",
        "\n",
        "        return content_data\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error scraping content: {e}\")\n",
        "        return {'title': '', 'sections': [], 'full_text': ''}\n",
        "\n",
        "# Scrape PydanticAI documentation\n",
        "pydantic_url = \"https://ai.pydantic.dev/#next-steps\"\n",
        "scraped_content = scrape_pydantic_ai_content(pydantic_url)\n",
        "\n",
        "# Display scraped content summary\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"üìã SCRAPED CONTENT SUMMARY\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Title: {scraped_content['title']}\")\n",
        "print(f\"Number of sections: {len(scraped_content['sections'])}\")\n",
        "print(f\"Total content length: {len(scraped_content['full_text'])} characters\")\n",
        "\n",
        "# Show first few sections\n",
        "print(\"\\nüìñ First 3 sections:\")\n",
        "for i, section in enumerate(scraped_content['sections'][:3]):\n",
        "    print(f\"\\n{i+1}. {section['heading']}\")\n",
        "    print(f\"   Content preview: {section['content'][:200]}...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59ZFjg5SvP-J",
        "outputId": "4d0a23fc-0b64-4495-9004-ba2a0bde4e38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ Scraping content from: https://ai.pydantic.dev/#next-steps\n",
            "‚úÖ Successfully scraped 7 sections\n",
            "üìÑ Total content length: 12167 characters\n",
            "\n",
            "==================================================\n",
            "üìã SCRAPED CONTENT SUMMARY\n",
            "==================================================\n",
            "Title: PydanticAI\n",
            "Number of sections: 7\n",
            "Total content length: 12167 characters\n",
            "\n",
            "üìñ First 3 sections:\n",
            "\n",
            "1. Introduction\n",
            "   Content preview: Agent Framework / shim to use Pydantic with LLMs PydanticAI is a Python agent framework designed to make it less painful to\n",
            "  build production grade applications with Generative AI. FastAPI revolution...\n",
            "\n",
            "2. Why use PydanticAI\n",
            "   Content preview: Built by the Pydantic Team:\n",
            "Built by the team behindPydantic(the validation layer of the OpenAI SDK, the Anthropic SDK, LangChain, LlamaIndex, AutoGPT, Transformers, CrewAI, Instructor and many more)....\n",
            "\n",
            "3. Hello World Example\n",
            "   Content preview: Here's a minimal example of PydanticAI: hello_world.pyfrompydantic_aiimportAgentagent=Agent(# (1)!'google-gla:gemini-1.5-flash',system_prompt='Be concise, reply with one sentence.',# (2)!)result=agent...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DOCUMENT PROCESSING AND CHUNKING"
      ],
      "metadata": {
        "id": "YKV2t_RPo2Y6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chunk_text(text: str, chunk_size: int = 1000, overlap: int = 200) -> List[str]:\n",
        "    \"\"\"\n",
        "    Split text into overlapping chunks for better retrieval\n",
        "    \"\"\"\n",
        "    chunks = []\n",
        "    start = 0\n",
        "\n",
        "    while start < len(text):\n",
        "        end = start + chunk_size\n",
        "\n",
        "        # Try to break at sentence boundary\n",
        "        if end < len(text):\n",
        "            last_period = text.rfind('.', start, end)\n",
        "            if last_period != -1 and last_period > start + chunk_size // 2:\n",
        "                end = last_period + 1\n",
        "\n",
        "        chunk = text[start:end].strip()\n",
        "        if chunk:\n",
        "            chunks.append(chunk)\n",
        "\n",
        "        start = end - overlap\n",
        "\n",
        "    return chunks\n",
        "\n",
        "# Process and chunk the content\n",
        "print(\"\\nüîÑ Processing and chunking content...\")\n",
        "\n",
        "# Combine all sections into documents\n",
        "documents = []\n",
        "for section in scraped_content['sections']:\n",
        "    # Create a document with heading and content\n",
        "    doc_text = f\"{section['heading']}\\n{section['content']}\"\n",
        "    documents.append({\n",
        "        'text': doc_text,\n",
        "        'heading': section['heading'],\n",
        "        'content': section['content']\n",
        "    })\n",
        "\n",
        "# Chunk documents for better retrieval\n",
        "all_chunks = []\n",
        "for doc in documents:\n",
        "    chunks = chunk_text(doc['text'], chunk_size=800, overlap=150)\n",
        "    for chunk in chunks:\n",
        "        all_chunks.append({\n",
        "            'text': chunk,\n",
        "            'source': doc['heading']\n",
        "        })\n",
        "\n",
        "print(f\"‚úÖ Created {len(all_chunks)} chunks from {len(documents)} documents\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GBQFyPmtvQI8",
        "outputId": "7eb44887-7c1f-4045-c432-53da0f265667"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîÑ Processing and chunking content...\n",
            "‚úÖ Created 19 chunks from 7 documents\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EMBEDDING GENERATION AND VECTOR STORE (WITH FALLBACK)"
      ],
      "metadata": {
        "id": "MowlkXs1o_tD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_embedding(text: str) -> List[float]:\n",
        "    \"\"\"\n",
        "    Generate embeddings using Ollama or fallback model\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Try Ollama first\n",
        "        response = ollama.embeddings(model='nomic-embed-text', prompt=text)\n",
        "        return response['embedding']\n",
        "    except Exception as ollama_error:\n",
        "        try:\n",
        "            # Use fallback model\n",
        "            if 'fallback_model' in globals():\n",
        "                embedding = fallback_model.encode(text)\n",
        "                return embedding.tolist()\n",
        "            else:\n",
        "                print(f\"No embedding model available: {ollama_error}\")\n",
        "                return []\n",
        "        except Exception as fallback_error:\n",
        "            print(f\"Fallback embedding error: {fallback_error}\")\n",
        "            return []\n",
        "\n",
        "# Generate embeddings for all chunks\n",
        "print(\"\\nüîÑ Generating embeddings for all chunks...\")\n",
        "\n",
        "chunk_embeddings = []\n",
        "valid_chunks = []\n",
        "\n",
        "for i, chunk in enumerate(all_chunks):\n",
        "    print(f\"Processing chunk {i+1}/{len(all_chunks)}\", end='\\r')\n",
        "\n",
        "    embedding = get_embedding(chunk['text'])\n",
        "    if embedding:\n",
        "        chunk_embeddings.append(embedding)\n",
        "        valid_chunks.append(chunk)\n",
        "\n",
        "print(f\"\\n‚úÖ Generated {len(chunk_embeddings)} embeddings\")\n",
        "\n",
        "if len(chunk_embeddings) == 0:\n",
        "    print(\"‚ùå No embeddings generated. Please check your setup.\")\n",
        "    print(\"üí° Trying alternative approach...\")\n",
        "\n",
        "    # Create dummy embeddings for demonstration\n",
        "    print(\"Creating demonstration embeddings...\")\n",
        "    import random\n",
        "\n",
        "    for chunk in all_chunks[:10]:  # Limit to first 10 chunks\n",
        "        # Create random embedding for demonstration\n",
        "        dummy_embedding = [random.random() for _ in range(384)]\n",
        "        chunk_embeddings.append(dummy_embedding)\n",
        "        valid_chunks.append(chunk)\n",
        "\n",
        "    print(f\"‚úÖ Created {len(chunk_embeddings)} demonstration embeddings\")\n",
        "\n",
        "# Convert to numpy array for faster similarity computation\n",
        "embeddings_matrix = np.array(chunk_embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hu7smdLKvlmF",
        "outputId": "585096fb-7a3f-459c-e4fc-0441e1ab7309"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîÑ Generating embeddings for all chunks...\n",
            "\n",
            "‚úÖ Generated 19 embeddings\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RETRIEVAL SYSTEM"
      ],
      "metadata": {
        "id": "BPCFFOPbpIr5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_relevant_chunks(query: str, top_k: int = 3) -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Retrieve most relevant chunks for a given query\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Generate query embedding\n",
        "        query_embedding = get_embedding(query)\n",
        "        if not query_embedding:\n",
        "            print(\"‚ùå Could not generate query embedding\")\n",
        "            return []\n",
        "\n",
        "        # Calculate similarities\n",
        "        query_vec = np.array(query_embedding).reshape(1, -1)\n",
        "        similarities = cosine_similarity(query_vec, embeddings_matrix)[0]\n",
        "\n",
        "        # Get top-k most similar chunks\n",
        "        top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
        "\n",
        "        results = []\n",
        "        for idx in top_indices:\n",
        "            results.append({\n",
        "                'text': valid_chunks[idx]['text'],\n",
        "                'source': valid_chunks[idx]['source'],\n",
        "                'similarity': similarities[idx]\n",
        "            })\n",
        "\n",
        "        return results\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in retrieval: {e}\")\n",
        "        return []\n",
        "\n",
        "# Test retrieval system\n",
        "print(\"\\nüîç Testing retrieval system...\")\n",
        "test_query = \"What is PydanticAI?\"\n",
        "test_results = retrieve_relevant_chunks(test_query)\n",
        "\n",
        "print(f\"Query: {test_query}\")\n",
        "print(f\"Retrieved {len(test_results)} relevant chunks:\")\n",
        "for i, result in enumerate(test_results):\n",
        "    print(f\"\\n{i+1}. Source: {result['source']}\")\n",
        "    print(f\"   Similarity: {result['similarity']:.4f}\")\n",
        "    print(f\"   Text: {result['text'][:200]}...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8O664ngvlwo",
        "outputId": "9f4a5c6f-0bc3-4a56-efca-f815a72c2a1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîç Testing retrieval system...\n",
            "Query: What is PydanticAI?\n",
            "Retrieved 3 relevant chunks:\n",
            "\n",
            "1. Source: Next Steps\n",
            "   Similarity: 0.7647\n",
            "   Text: Next Steps\n",
            "To try PydanticAI yourself, follow the instructionsin the examples. Read thedocsto learn more about building applications with PydanticAI. Read theAPI Referenceto understand PydanticAI's in...\n",
            "\n",
            "2. Source: Why use PydanticAI\n",
            "   Similarity: 0.7108\n",
            "   Text: Why use PydanticAI\n",
            "Built by the Pydantic Team:\n",
            "Built by the team behindPydantic(the validation layer of the OpenAI SDK, the Anthropic SDK, LangChain, LlamaIndex, AutoGPT, Transformers, CrewAI, Instruc...\n",
            "\n",
            "3. Source: Introduction\n",
            "   Similarity: 0.7031\n",
            "   Text: Introduction\n",
            "Agent Framework / shim to use Pydantic with LLMs PydanticAI is a Python agent framework designed to make it less painful to\n",
            "  build production grade applications with Generative AI. FastA...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM RESPONSE GENERATION"
      ],
      "metadata": {
        "id": "IQsF9r2LpPly"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_response(query: str, context_chunks: List[Dict[str, Any]]) -> str:\n",
        "    \"\"\"\n",
        "    Generate response using Ollama LLM with retrieved context\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Prepare context\n",
        "        context = \"\\n\\n\".join([f\"Context {i+1}:\\n{chunk['text']}\" for i, chunk in enumerate(context_chunks)])\n",
        "\n",
        "        # Create prompt\n",
        "        prompt = f\"\"\"You are a helpful assistant that answers questions about PydanticAI based on the provided context.\n",
        "\n",
        "Context Information:\n",
        "{context}\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Please provide a comprehensive answer based on the context provided. If the information is not available in the context, please say so.\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "        # Try to generate response using Ollama\n",
        "        try:\n",
        "            response = ollama.generate(\n",
        "                model='llama3.2:3b',\n",
        "                prompt=prompt,\n",
        "                options={\n",
        "                    'temperature': 0.7,\n",
        "                    'max_tokens': 500,\n",
        "                    'top_p': 0.9\n",
        "                }\n",
        "            )\n",
        "            return response['response']\n",
        "\n",
        "        except Exception as ollama_error:\n",
        "            print(f\"‚ùå Ollama LLM error: {ollama_error}\")\n",
        "\n",
        "            # Fallback to basic response\n",
        "            if context_chunks:\n",
        "                return f\"Based on the available context about PydanticAI: {context_chunks[0]['text'][:300]}...\"\n",
        "            else:\n",
        "                return \"I apologize, but I couldn't find relevant information to answer your question about PydanticAI.\"\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating response: {e}\")\n",
        "        return \"I apologize, but I encountered an error while generating a response.\""
      ],
      "metadata": {
        "id": "w95iAAOSvxE1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# COMPLETE FAQ BOT CLASS"
      ],
      "metadata": {
        "id": "Fn7hf7DwpZ46"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PydanticAIFAQBot:\n",
        "    def __init__(self):\n",
        "        self.embeddings_matrix = embeddings_matrix\n",
        "        self.chunks = valid_chunks\n",
        "\n",
        "    def ask(self, question: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Ask a question to the FAQ bot\n",
        "        \"\"\"\n",
        "        print(f\"\\n‚ùì Question: {question}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        # Retrieve relevant context\n",
        "        relevant_chunks = retrieve_relevant_chunks(question, top_k=3)\n",
        "\n",
        "        if not relevant_chunks:\n",
        "            return {\n",
        "                'question': question,\n",
        "                'answer': \"I couldn't find relevant information to answer your question.\",\n",
        "                'sources': []\n",
        "            }\n",
        "\n",
        "        # Generate response\n",
        "        answer = generate_response(question, relevant_chunks)\n",
        "\n",
        "        # Prepare response\n",
        "        response = {\n",
        "            'question': question,\n",
        "            'answer': answer,\n",
        "            'sources': [chunk['source'] for chunk in relevant_chunks],\n",
        "            'relevance_scores': [chunk['similarity'] for chunk in relevant_chunks]\n",
        "        }\n",
        "\n",
        "        print(f\"ü§ñ Answer: {answer}\")\n",
        "        print(f\"\\nüìö Sources: {', '.join(response['sources'])}\")\n",
        "        print(f\"üéØ Relevance scores: {[f'{score:.3f}' for score in response['relevance_scores']]}\")\n",
        "\n",
        "        return response\n",
        "\n",
        "# Initialize the FAQ bot\n",
        "faq_bot = PydanticAIFAQBot()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ü§ñ PYDANTIC AI FAQ BOT INITIALIZED\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UB8Q1itnv7fy",
        "outputId": "f1a48ba8-3939-42ea-d3ed-fcac31b36633"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "ü§ñ PYDANTIC AI FAQ BOT INITIALIZED\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# QUESTION-ANSWERING TASK"
      ],
      "metadata": {
        "id": "nQQpy--PpfEy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the 5 conceptual questions about PydanticAI\n",
        "questions = [\n",
        "    \"What is PydanticAI and why was it created?\",\n",
        "    \"How is PydanticAI similar to FastAPI in terms of developer experience?\",\n",
        "    \"What makes PydanticAI type-safe and structured?\",\n",
        "    \"How does PydanticAI support streaming and debugging?\",\n",
        "    \"What is llms.txt?\"\n",
        "]\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üìù QUESTION-ANSWERING SESSION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Store all responses\n",
        "all_responses = []\n",
        "\n",
        "for i, question in enumerate(questions, 1):\n",
        "    print(f\"\\n{'='*20} QUESTION {i}/5 {'='*20}\")\n",
        "    response = faq_bot.ask(question)\n",
        "    all_responses.append(response)\n",
        "    print(\"\\n\" + \"=\"*60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PuFGp29xwKko",
        "outputId": "5c647a4a-4bf7-4709-fd5d-b65b92279bc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "üìù QUESTION-ANSWERING SESSION\n",
            "============================================================\n",
            "\n",
            "==================== QUESTION 1/5 ====================\n",
            "\n",
            "‚ùì Question: What is PydanticAI and why was it created?\n",
            "--------------------------------------------------\n",
            "ü§ñ Answer: Based on the context provided, PydanticAI appears to be a Python agent framework designed to make it easier to build production-grade applications with Generative AI (LLMs). According to Context 3, PydanticAI was created with the aim of bringing the same feeling and ergonomic design experience that FastAPI offers to GenAI app development.\n",
            "\n",
            "While the context does not explicitly state what PydanticAI is or its full purpose, it can be inferred that it is a framework built on top of Pydantic, which is a popular validation layer used in various OpenAI, Anthropic, and other AI-related SDKs. By leveraging Pydantic's strengths, PydanticAI aims to simplify the process of developing applications with LLMs, making it more accessible and user-friendly for developers.\n",
            "\n",
            "In summary, PydanticAI was created to provide a Python agent framework that enables easy and efficient development of production-grade applications using Generative AI (LLMs), drawing inspiration from FastAPI's design principles.\n",
            "\n",
            "üìö Sources: Next Steps, Why use PydanticAI, Introduction\n",
            "üéØ Relevance scores: ['0.743', '0.730', '0.713']\n",
            "\n",
            "============================================================\n",
            "\n",
            "==================== QUESTION 2/5 ====================\n",
            "\n",
            "‚ùì Question: How is PydanticAI similar to FastAPI in terms of developer experience?\n",
            "--------------------------------------------------\n",
            "ü§ñ Answer: Based on the provided context, it can be inferred that PydanticAI shares similarities with FastAPI in terms of developer experience. The context states:\n",
            "\n",
            "\"FastAPI revolutionized web development by offering an innovative and ergonomic design, built on the foundation ofPydantic.\"\n",
            "\n",
            "This suggests that PydanticAI builds upon the same foundation as FastAPI, leveraging its strengths to create a similar developer experience for building applications with Generative AI.\n",
            "\n",
            "In terms of specifics, the context mentions that PydanticAI was designed to \"bring that FastAPI feeling\" to GenAI app development. This implies that PydanticAI aims to provide an equally innovative and ergonomic design, allowing developers to focus on building their applications without getting bogged down in tedious setup or configuration.\n",
            "\n",
            "The context also highlights Pydantic's strengths, such as:\n",
            "\n",
            "* Being type-safe\n",
            "* Providing a powerful validation layer\n",
            "* Seamlessly integrating with other tools and libraries (e.g., OpenAI SDK)\n",
            "\n",
            "These features are likely to contribute to a similar developer experience for PydanticAI as FastAPI does. However, without explicit details on how PydanticAI specifically addresses certain aspects of developer experience, it is difficult to provide a more detailed answer.\n",
            "\n",
            "Overall, while the context does not offer an exhaustive comparison between PydanticAI and FastAPI, it suggests that PydanticAI aims to replicate or build upon the strengths of FastAPI in terms of developer experience.\n",
            "\n",
            "üìö Sources: Introduction, Next Steps, Why use PydanticAI\n",
            "üéØ Relevance scores: ['0.758', '0.757', '0.721']\n",
            "\n",
            "============================================================\n",
            "\n",
            "==================== QUESTION 3/5 ====================\n",
            "\n",
            "‚ùì Question: What makes PydanticAI type-safe and structured?\n",
            "--------------------------------------------------\n",
            "ü§ñ Answer: Based on the provided context, it can be inferred that PydanticAI is designed to be type-safe and structured due to its integration with Pydantic, which is already a type-safe framework for building web applications.\n",
            "\n",
            "Pydantic's design goal is \"to make type checking as powerful and informative as possible\", suggesting that it provides strong support for type safety. Given that PydanticAI builds upon this foundation, it can be inferred that PydanticAI also prioritizes type safety and structure in its design.\n",
            "\n",
            "Furthermore, the fact that PydanticAI is built by the same team behind Pydantic suggests a deep understanding of the existing framework's principles and goals, which would likely extend to maintaining type safety and structure in PydanticAI as well.\n",
            "\n",
            "However, without explicit information from the context stating that PydanticAI explicitly incorporates additional features or mechanisms for type safety and structure beyond what is available in Pydantic, it cannot be confirmed whether PydanticAI provides any novel or enhanced features in this regard.\n",
            "\n",
            "In summary, while the exact nature of PydanticAI's type safety and structure is not specified in the provided context, it can be inferred that PydanticAI likely inherits many of the same design principles and features from its parent framework, Pydantic.\n",
            "\n",
            "üìö Sources: Why use PydanticAI, Next Steps, Introduction\n",
            "üéØ Relevance scores: ['0.769', '0.673', '0.661']\n",
            "\n",
            "============================================================\n",
            "\n",
            "==================== QUESTION 4/5 ====================\n",
            "\n",
            "‚ùì Question: How does PydanticAI support streaming and debugging?\n",
            "--------------------------------------------------\n",
            "ü§ñ Answer: Based on the provided context, it appears that PydanticAI supports streaming and debugging through its integration with Pydantic Logfire. According to Context 1:\n",
            "\n",
            "\"Pydantic Logfire Integration: Seamlessly integrates with Pydantic Logfire for real-time debugging, performance monitoring, and behavior tracking of your LLM-powered applications.\"\n",
            "\n",
            "This suggests that PydanticAI provides a way to stream LLM responses continuously, with immediate validation, which is useful for testing and eval-driven iterative development. The integration with Pydantic Logfire also enables real-time debugging, performance monitoring, and behavior tracking, making it easier to identify and resolve issues in the application.\n",
            "\n",
            "Additionally, the context mentions that PydanticAI provides streamed responses, which implies that it can handle continuous streaming of LLM responses without interruption or delay. This feature is particularly useful for applications where immediate access to validated outputs is necessary.\n",
            "\n",
            "While the context does not provide detailed information on how PydanticAI implements streaming and debugging, it highlights the importance of these features in supporting its overall functionality and user experience.\n",
            "\n",
            "üìö Sources: Why use PydanticAI, Next Steps, Why use PydanticAI\n",
            "üéØ Relevance scores: ['0.714', '0.681', '0.641']\n",
            "\n",
            "============================================================\n",
            "\n",
            "==================== QUESTION 5/5 ====================\n",
            "\n",
            "‚ùì Question: What is llms.txt?\n",
            "--------------------------------------------------\n",
            "ü§ñ Answer: Based on the provided context, llms.txt appears to be a file format used for storing documentation related to PydanticAI. According to Context 1, there are two formats available:\n",
            "\n",
            "1. `llms.txt`: A brief description of the project, along with links to different sections of the documentation.\n",
            "2. `llms-full.txt`: Similar to `llms.txt`, but every link content is included.\n",
            "\n",
            "It seems that `llms.txt` is a more compact version of the documentation, containing only the essential information and links to other sections, while `llms-full.txt` includes all the details, including the actual links. However, due to its size, `llms-full.txt` may not be natively leveraged by LLM frameworks or IDEs.\n",
            "\n",
            "Unfortunately, there is no further explanation of what `llms.txt` specifically means in terms of its content or usage. It appears to be a format used for organizing and presenting documentation related to PydanticAI.\n",
            "\n",
            "üìö Sources: llms.txt, Why use PydanticAI, Tools & Dependency Injection Example\n",
            "üéØ Relevance scores: ['0.798', '0.576', '0.558']\n",
            "\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SUMMARY AND RESULTS"
      ],
      "metadata": {
        "id": "1eWo88ewplgj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üìä SUMMARY OF FAQ BOT PERFORMANCE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(f\"‚úÖ Successfully scraped content from: {pydantic_url}\")\n",
        "print(f\"‚úÖ Processed {len(documents)} documents into {len(all_chunks)} chunks\")\n",
        "print(f\"‚úÖ Generated {len(chunk_embeddings)} embeddings using 'nomic-embed-text'\")\n",
        "print(f\"‚úÖ Used 'llama3.2:3b' model for response generation\")\n",
        "print(f\"‚úÖ Answered {len(questions)} questions successfully\")\n",
        "\n",
        "print(\"\\nüìã MODELS USED:\")\n",
        "print(\"- LLM Model: llama3.2:3b (for text generation)\")\n",
        "print(\"- Embedding Model: nomic-embed-text (for semantic search)\")\n",
        "\n",
        "print(\"\\nüîß TECHNICAL APPROACH:\")\n",
        "print(\"- Content Scraping: BeautifulSoup for HTML parsing\")\n",
        "print(\"- Text Processing: Chunking with overlap for better retrieval\")\n",
        "print(\"- Embedding Generation: Ollama embedding API\")\n",
        "print(\"- Similarity Search: Cosine similarity for relevant chunk retrieval\")\n",
        "print(\"- Response Generation: Context-aware prompting with Ollama LLM\")\n",
        "\n",
        "print(\"\\nüí° AVERAGE RELEVANCE SCORES:\")\n",
        "for i, response in enumerate(all_responses, 1):\n",
        "    avg_score = np.mean(response['relevance_scores'])\n",
        "    print(f\"Question {i}: {avg_score:.3f}\")\n",
        "\n",
        "print(\"\\nüéâ FAQ Bot setup and testing completed successfully!\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WkdYQGGLwK8j",
        "outputId": "ab7c50a9-c8d8-4c8d-eed4-71f195727183"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "üìä SUMMARY OF FAQ BOT PERFORMANCE\n",
            "============================================================\n",
            "‚úÖ Successfully scraped content from: https://ai.pydantic.dev/#next-steps\n",
            "‚úÖ Processed 7 documents into 19 chunks\n",
            "‚úÖ Generated 19 embeddings using 'nomic-embed-text'\n",
            "‚úÖ Used 'llama3.2:3b' model for response generation\n",
            "‚úÖ Answered 5 questions successfully\n",
            "\n",
            "üìã MODELS USED:\n",
            "- LLM Model: llama3.2:3b (for text generation)\n",
            "- Embedding Model: nomic-embed-text (for semantic search)\n",
            "\n",
            "üîß TECHNICAL APPROACH:\n",
            "- Content Scraping: BeautifulSoup for HTML parsing\n",
            "- Text Processing: Chunking with overlap for better retrieval\n",
            "- Embedding Generation: Ollama embedding API\n",
            "- Similarity Search: Cosine similarity for relevant chunk retrieval\n",
            "- Response Generation: Context-aware prompting with Ollama LLM\n",
            "\n",
            "üí° AVERAGE RELEVANCE SCORES:\n",
            "Question 1: 0.729\n",
            "Question 2: 0.746\n",
            "Question 3: 0.701\n",
            "Question 4: 0.678\n",
            "Question 5: 0.644\n",
            "\n",
            "üéâ FAQ Bot setup and testing completed successfully!\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OPTIONAL: INTERACTIVE TESTING"
      ],
      "metadata": {
        "id": "RbecQdEmpq5r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fAeqyRN-uWHl",
        "outputId": "2a0aa222-69f5-48fb-d996-8d9d9a8d427f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîß FAQ Bot is ready for interactive use!\n",
            "You can now ask questions about PydanticAI using: faq_bot.ask('your question here')\n",
            "\n",
            "üí° Example of additional usage:\n",
            "\n",
            "‚ùì Question: How does PydanticAI handle type validation?\n",
            "--------------------------------------------------\n",
            "ü§ñ Answer: Based on the provided context, PydanticAI handles type validation through its design as a Type-safe model. The context states that PydanticAI \"Designed to make type checking as powerful and informative as possible for you.\" This suggests that PydanticAI has been built with type safety in mind, allowing users to take advantage of powerful and informative type checking.\n",
            "\n",
            "Furthermore, the context mentions that the agent is typed as a `SupportOutput`, which implies that PydanticAI provides strong typing and validation capabilities. Additionally, it's mentioned that if validation fails, reflection will occur, and the agent will be prompted to try again, indicating that PydanticAI has robust type checking mechanisms in place.\n",
            "\n",
            "However, it's not explicitly stated how PydanticAI handles type validation in terms of specific algorithms or techniques used. The context only mentions that it's designed to make type checking as powerful and informative as possible, but doesn't provide further details on the underlying implementation.\n",
            "\n",
            "In summary, based on the provided context, PydanticAI appears to have robust type validation capabilities through its design as a Type-safe model, allowing users to take advantage of powerful and informative type checking.\n",
            "\n",
            "üìö Sources: Why use PydanticAI, Tools & Dependency Injection Example, Next Steps\n",
            "üéØ Relevance scores: ['0.741', '0.676', '0.656']\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nüîß FAQ Bot is ready for interactive use!\")\n",
        "print(\"You can now ask questions about PydanticAI using: faq_bot.ask('your question here')\")\n",
        "\n",
        "# Example of additional usage\n",
        "print(\"\\nüí° Example of additional usage:\")\n",
        "example_response = faq_bot.ask(\"How does PydanticAI handle type validation?\")"
      ]
    }
  ]
}